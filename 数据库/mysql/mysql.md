# mysql

## mysql整体结构

MySQL可以分为**Server层**和**存储引擎层**两部分。

​	Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大多数核心服务
功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在
这一层实现，比如存储过程、触发器、视图等。

​	而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持InnoDB、MyISAM、
Memory等多个存储引擎。现在最常用的存储引擎是InnoDB，它从MySQL 5.5.5版本开始成为了
默认存储引擎。

![1677908702316](C:\Users\Chenhui\AppData\Roaming\Typora\typora-user-images\1677908702316.png)

​	数据库里面，**长连接**是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。**短连接**
则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。

​	MySQL在执行过程中**临时使用的内存**是管理在连接对象里面的。这些资源会**在连接断开的时候**
**才释放**。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现
象看就是MySQL异常重启了。

### 查询缓存

​	MySQL拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以key-value对的形式，被直接缓存在内存中。key是查询的语句，value是查询的结果。

​	但是大多数情况下我会建议你不要使用查询缓存，为什么呢？因为**查询缓存往往弊大于利**。原因在于查询缓存的**失效非常频繁**，只要有对一个表的更新，这个表上所有的查询缓存都会被清空（更新会导致缓存失效）。

​	好在MySQL也提供了这种“**按需使用**”的方式。你可以将参数query_cache_type设置成DEMAND，这样对于默认的SQL语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用SQL_CACHE显式指定，像下面这个语句一样：

```mysql
mysql> select SQL_CACHE * from T where ID=10;
```

### 分析器

1. 分析器先会做“**词法分析**”。你输入的是由多个字符串和空格组成的一条SQL语句，MySQL需要识别出里面的字符串分别是什么，代表什么。
2. 做完了这些识别以后，就要做“**语法分析**”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个SQL语句是否满足MySQL语法。如果你的语句不对，就会收到“You have an error in your SQL syntax”的错误提醒。

### 优化器

​	优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。

### 执行器

​	开始执行的时候，要先判断一下你对这个表T有没有执行查询的权限，如果没有，就会返回没有权限的错误，如下所示。

```mysql
mysql> select * from T where ID=10;
ERROR 1142 (42000): SELECT command denied to user 'b'@'localhost' for table 'T'
```

如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。
比如我们这个例子中的表T中，ID字段没有索引，那么执行器的执行流程是这样的：

1. 调用InnoDB引擎接口取这个表的第一行，判断ID值是不是10，如果不是则跳过，如果是则
   将这行存在结果集中；
2. 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。
3. 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。
   至此，这个语句就执行完成了。



## 更新语句如何执行

​	与查询流程不一样的是，更新流程还涉及两个重要的日志模块，分别是**redo log（重做日志）**和**binlog（归档日志）**。

### redo log

​	有点像消息队列的概念，把要操作的更新操作先缓存到redo log中，等空闲时间后再落到磁盘中。这是**InnoDB引擎特有的日志**。

#### 特点

1. 有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为**crash-safe**(根据redo log和重启后数据库状态来恢复)。
2. InnoDB的redo log是固定大小的，所以是循环写入的。比如可以配置为一组4个文件，每个文件的大小是1GB，总共就可以记录4GB的操作。redo log是

### bin log

​	归档日志，MySQL的Server层实现的，所有引擎都可以使用。



### 不同点

这两种日志有以下三点不同。

1. redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。
2. redo log**是物理日志**，记录的是“在某个数据页上做了什么修改”；binlog是**逻辑日志**（记录的历史操作），记录的
   是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。
3. redo log是**循环写的**，空间固定会用完；binlog是可以**追加写入**的。“追加写”是指binlog文件
   写到一定大小后会切换到下一个，并不会覆盖以前的日志。



### 用途

1. redo log提供了crash-safe的能力，即使数据库宕机，仍然能够恢复。
2. bin log提供了回退到某个历史状态的能力



​	当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数据，那你可以这么做：首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库；然后，从备份的时间点开始，将备份的binlog依次取出来，重放到中午误删表之前的那个时刻。

**数据恢复过程**

前提：

1. 定时整库备份  
2. 保存一定时间内的binlog

​	当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数据，那你可以这么做：首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库；然后，从备份的时间点开始，将备份的binlog依次取出来，重放到中午误删表之前的那个时刻。



### 更新数据过程

​	从执行流程可以看到，mysql通过两阶段提交的方式来更新数据，这是为了让两份日志之间的逻辑一致。

![1678345579126](C:\Users\Chenhui\AppData\Roaming\Typora\typora-user-images\1678345579126.png)

### 总结

1. `	innodb_flush_log_at_trx_commit`这个参数设置成1的时候，表示每次事务的redo log都直接持久化到磁盘。这个参数建议设置成1，这样可以保证MySQL异常重启之后数据不丢失。
2. `sync_binlog`这个参数设置成1的时候，表示每次事务的binlog都持久化到磁盘。这个参数也建
   议设置成1，这样可以保证MySQL异常重启之后binlog不丢失。



## 数据库事务

​	事务就是要保证一组数据库操作，要么全部成功，要么全部失败。在MySQL中，**事务支持是在引擎层实现的**。MySQL是一个支持多引擎的系统，但并非所有的引擎都支持事务。比如MySQL原生的**MyISAM引擎就不支持事务**，这也是MyISAM被InnoDB取代的重要原因之一。

### 隔离性和隔离级别

1. 读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。
2. 读提交是指，一个事务提交之后，它做的变更才会被其他事务看到。
3. 可重复读是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一
   致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。
4. 串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突
   的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。

​	在实现上，**数据库里面会创建一个视图**，访问的时候以视图的逻辑结果为准。在“**可重复读**”隔离
级别下，这个视图是在**事务启动时创建**的，整个事务存在期间都用这个视图。在“**读提交**”隔离级
别下，这个视图是在**每个SQL语句开始执行**的时候创建的。这里需要注意的是，“读未提交”隔离
级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避
免并行访问。

### 事务隔离的实现

​	在MySQL中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通
过回滚操作，都可以得到前一个状态的值。

​	但是在查询这条记录的时候，不同时刻启动的事务会有不同的read-view。如图中看到的，在视图A、B、C里面，这一个记录的值分别是1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。对于read-viewA，要得到1，就必须将当前值依次执行图中所有的回滚操作得。到。

![1678348908609](C:\Users\Chenhui\AppData\Roaming\Typora\typora-user-images\1678348908609.png)

​	**回滚日志在不需要的时候才删除**。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。

什么时候才不需要了呢？就是当系统里没有比这个回滚日志更早的read-view的时候。

​	**不要使用长事务**，长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。



## 数据库索引

索引的出现其实就是为了提高数据查询的效率，就像书的目录一样。

### 常见索引类型

常见的索引数据结构有哈希表、有序数组和搜索树

​	索引不止存在内存中，还要写到磁盘上。**为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。**N叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中了（[B树和B+数](https://github.com/wardseptember/notes/blob/master/docs/B%E6%A0%91%E5%92%8CB%2B%E6%A0%91%E8%AF%A6%E8%A7%A3.md)）。

​	磁盘读取数据靠的是机械运动，每次读取数据花费的时间可以分为**寻道时间、旋转延迟、传输时间**三个部分。其中寻到时间和旋转延迟是非常耗时的，因此当一次IO时，会**预读数据**。每一次IO读取的数据我们称之为一页(page)，一般为4k或8k，也就是我们读取一页内的数据时候，实际上才发生了一次IO。

​	索引查询的数据主要受限于硬盘的I/O速度，查询I/O次数越少，速度越快，所以B树的结构才应需求而生；B树的每个节点的元素可以视为一次I/O读取，树的高度表示最多的I/O次数，在相同数量的总元素个数下，每个节点的元素个数越多，高度越低，查询所需的I/O次数越少；假设，一次硬盘一次I/O数据为8K，索引用int(4字节)类型数据建立，理论上一个节点最多可以为2000个元素`2000*2000*2000=8000000000`，80亿条的数据只需3次I/O（理论值），可想而知，B树做为索引的查询效率有多高；另外也可以看出**同样的总元素个数，查询效率和树的高度密切相关**

**B树的高度**

一棵含有N个总关键字数的m阶的B树的最大高度是多少？

log（m/2）(N+1)/2 + 1 ，log以（m/2）为低，(N+1)/2的对数再加1

### InnoDB 的索引模型

​	在InnoDB中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。
又因为前面我们提到的，InnoDB使用了**B+树索引模型**，所以数据都是存储在B+树中的。**每一个索引在InnoDB里面对应一棵B+树**。

#### 主键索引和主键索引

​	主键索引的叶子节点存的是**整行数据**。在InnoDB里，主键索引也被称为聚簇索引（clustered
index）。
​	非主键索引的叶子节点内容是**主键的值**。在InnoDB里，非主键索引也被称为二级索引
（secondary index）。

​	**基于非主键索引查询，首先会根据该索引查询主键的信息；然后再从主键索引查找一次。**

在流程中从非主键索引树搜索回到主键索引树搜索的过程称为：**回表**

#### 索引维护

​	B+树为了维护索引有序性，在插入新值的时候需要做必要的维护。这里存在**页分裂**和**页合并**的情况，由于InnoDB是索引组织表，一般情况下我会建议你创建一个自增主键，这样非主键索引占用的空间最小。

有些业务的场景需求（典型的KV场景）场景适合用业务字段直接做主键，其特点如下：

1. 只有一个索引；
2. 该索引必须是唯一索引。



#### 使用B+或B树的原因

​	索引存在磁盘上，磁盘I/O是一件非常耗时的操作，磁盘的每一次I/O通过按页来进行预读数据。索引查询的数据主要受限于硬盘的I/O速度，查询I/O次数越少，速度越快。**如果在二叉树的每一个节点存储更多的有用元素，就可以充分利用预读减少I/O操作。**



##### **为什么说B+树比B树更适合数据库索引？**

1）B+树的磁盘读写代价更低

**B+树的内部结点并没有指向关键字具体信息的指针**。因此其内部结点相对B 树更小。如果把所有同一内部结点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多。一次性读入内存中的需要查找的关键字也就越多。相对来说IO读写次数也就降低了；

2）B+树查询效率更加稳定

由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。**所有关键字查询的路径长度相同**，导致每一个数据的查询效率相当；

3）B+树便于范围查询（最重要的原因，范围查找是数据库的常态）

B树在提高了IO性能的同时并没有解决元素遍历的我效率低下的问题，正是为了解决这个问题，B+树应用而生。**B+树只需要去遍历叶子节点就可以实现整棵树的遍历**。而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作或者说效率太低；不懂可以看看这篇解读-》[范围查找](https://zhuanlan.zhihu.com/p/54102723)

补充：B树的范围查找用的是中序遍历，而B+树用的是在链表上遍历；



### 联合索引

**减少开销**。建一个联合索引 `(col1,col2,col3)`，实际相当于建了 `(col1)`，`(col1,col2)`，`(col1,col2,col3)` 三个索引。每多一个索引，都会增加写操作的开销和磁盘空间的开销。对于大量数据的表，使用联合索引会大大的减少开销！

**覆盖索引**。对联合索引 `(col1,col2,col3)`，如果有如下的 SQL：`select col1,col2,col3 from test where col1=1 and col2=2;`。那么 MySQL 可以直接通过遍历索引取得数据，而无需回表，这减少了很多的随机 IO 操作。减少 IO 操作，特别的随机 IO 其实是 DBA 主要的优化策略。所以，在真正的实际应用中，覆盖索引是主要的提升性能的优化手段之一。

**效率高**。索引列越多，通过索引筛选出的数据越少。有 1000W 条数据的表，有如下 SQL：`select from table where col1=1 and col2=2 and col3=3`，假设假设每个条件可以筛选出 10% 的数据，如果只有单值索引，那么通过该索引能筛选出 1000W10%=100w 条数据，然后再回表从 100w 条数据中找到符合 col2=2 and col3=3 的数据，然后再排序，再分页；如果是联合索引，通过索引筛选出 1000w * 10% * 10% * 10% =1w，效率提升可想而知！



#### [索引覆盖](https://juejin.cn/post/6844903967365791752)

​	如果在二级索引没有查到所需的信息，会通过回表的方式从主键索引获取。

​	**覆盖索引**（covering index ，或称为索引覆盖）即从非主键索引中就能查到的记录，而不需要查询主键索引中的记录，避免了回表的产生减少了树的搜索次数，显著提升性能。

​	**实现方式**：如果现在有一个高频请求，要根据市民的身份证号查询他的姓名，这个联合索引就有意义了。它可以在这个高频请求上用到覆盖索引，不再需要回表查整行记录，减少语句的执行时间。



总结：覆盖索引避免了回表现象的产生，从而减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是性能优化的一种手段。覆盖索引在建立冗余索引来支持覆盖索引时就需要权衡考虑了。



#### 最左前缀原则

​	**最左前缀原则**：即当你创建了一个联合索引，该索引的任何最左前缀都可以用于查询。比如当你有一个联合索引 ` (col1, col2, col3)`，该索引的所有前缀为 `(col1)`、`(col1, col2)`、`(col1, col2, col3)`，包含这些列的所有查询都会使用该索引进行查询。

在建立联合索引的时候，索引内的字段顺序，一般要考虑：

1. 如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。
2. 如果既有联合查询，又有基于a、b各自的查询。查询条件里面只有b的语句，是无法使
   用(a,b)这个联合索引的，这时不得不维护另外一个索引，也就是说需要同时维护(a,b)、
   (b) 这两个索引。这就需要考虑建（a)和（b)哪种开销小了。



#### 索引下推

​	可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。



例子：

​	以市民表的联合索引（name, age）为例。如果现在有一个需求：检索出表中“名字第一个字是张，而且年龄是10岁的所有男孩”。那么，SQL语句是这么写的：

```mysql
mysql> select * from tuser where name like '张%' and age=10 and ismale=1;
```

InnoDB在(name,age)索引内部就判断了age是否等于10，对于不等于10的记录，直接判断并跳过。这样子可以减少回表的次数。

不含索引瞎推：回表4次

![1678372694649](C:\Users\Chenhui\AppData\Roaming\Typora\typora-user-images\1678372694649.png)

索引下推：只回表2次

![1678372743503](C:\Users\Chenhui\AppData\Roaming\Typora\typora-user-images\1678372743503.png)



### 前缀索引

​	MySQL是支持前缀索引的，也就是说，可以**定义字符串的一部分作为索引**。

​	使用前缀索引可能会导致匹配的行数增加，但是定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。

​	使用前缀索引就用不上覆盖索引对查询性能的优化了（**索引前缀没有包含完整的信息**），这也是在选择是否使用前缀索引时需要考虑的一个因素。



#### 该使用多长的前缀

​	在建立索引时关注的是区分度，**区分度越高越好**。因为区分度越高，意味着重复的键值越少。因此，可以通过**统计索引上有多少个不同的值**来判断要使用多长的前缀。

```mysql
select count(distinct email) as L from SUser;
```



#### 字符串字段创建索引

使用的方式有：

1. 直接创建完整索引，这样可能比较占用空间；
2. 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引；
3. 倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题；
4. 创建hash字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支
   持范围扫描。



### 唯一索引和普通索引

#### 查询过程

- 对于普通索引来说，查找到满足条件的第一个记录(5,500)后，需要查找下一个记录，直到碰到第一个不满足k=5条件的记录。
- 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。



​	**引擎是按页读写**的，它所在的数据页就都在内存里了。对于普通索引来说，要多做的那一次“查找和判断下一条记录”的操作，就只需要一次指针寻找和一次计算。**因此两者在查询上性能接近**。



#### 更新过程

##### change buffer

​	当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB会将这些更新操作缓存在change buffer中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。

​	将change buffer中的操作应用到原数据页，得到最新结果的过程称为merge。除了访问这个数据页会触发merge外，系统有后台线程会定期merge。在数据库正常关闭（shutdown）的过程中，也会执行merge操作。	**merge操作并不会刷脏页到磁盘上。**

**merge的执行流程**

1. 从磁盘读入数据页到内存（老版本的数据页）；
2. 从change buffer里找出这个数据页的change buffer 记录(可能有多个），依次应用，得到新版数据页；
3. 写redo log。这个redo log包含了数据的变更和change buffer的变更。



##### 什么条件下可以使用change buffer

​	对于**唯一索引**来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要插入(4,400)这个记录，就要先判断现在表中是否已经存在k=4的记录，而这**必须要将数据页读入内存**才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用change buffer了。

​	因此，**唯一索引的更新就不能使用change buffer**，实际上也只有普通索引可以使用。

​	从更新角度来看，普通索引相较于唯一索引有更少的IO访问，性能更好。



##### change buffer在系统断电时会丢失吗？

​	在事务提交的时候，会把change buffer的操作也记录到redo log里了，所以崩溃恢复的时候，change buffer也能找回来。



##### change buffer使用场景

​	因为merge的时候是真正进行数据更新的时刻，而change buffer的**主要目的就是将记录的变更动作缓存下来**，所以在一个数据页做merge之前，change buffer记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。

​	对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时change buffer的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。



##### 总结

​	在实际使用中，普通索引和change buffer的配合使用，对于数据量大的表的更新优化还会很明显的。**建议优先使用普通索引。**



## 锁

### 全局锁

全局锁主要用在逻辑备份过程中。对于全部是InnoDB引擎的库，我建议你选择使用`-single-transaction`参数，对应用会更友好

### 表级锁

MySQL里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。

​	表锁一般是在数据库引擎不支持行锁的时候才会被用到的。如果你发现你的应用程序里有`lock tables`这样的语句，你需要追查一下，比较可能的情况是：

1. 要么是你的系统现在还在用MyISAM这类不支持事务的引擎，那要安排升级换引擎；
2. 要么是你的引擎升级了，但是代码还没升级。我见过这样的情况，最后业务开发就是把`lock tables `和 `unlock tables` 改成` begin` 和 `commit`，问题就解决了。



​	MDL不需要显式使用，在访问一个表的时候会被自动加上。MMDL锁会直到事务提交才释放，在做表结构变更的时候，你一定要小心不要导致锁住线上查询和更新。

### 行锁

MySQL的行锁是在**引擎层**由各个引擎自己实现的。

在InnoDB事务中，行锁是在**需要的时候才加上**的，但并不是不需要了就立刻释放，而是要等到**事务结束时才释放**。这个就是两阶段锁协议。

#### 死锁

当出现死锁以后，有两种策略：

1. 直接进入等待，直到超时。这个超时时间可以通过参数`innodb_lock_wait_timeout`来设置。
2. 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数`innodb_deadlock_detect`设置为`on`，表示开启这个逻辑。



**减少死锁的主要方向，就是控制访问相同资源的并发事务量。**

#### 间隙锁（next-key lock，左开右闭）

只在可重复读和串行下才有间隙锁，**间隙锁是为了解决幻读而加的**。

##### 2个原则+2个优化+1个bug

1. 原则1：加锁的基本单位是next-key lock，next-key lock是前开后闭区间。
2. 原则2：查找过程中访问到的对象才会加锁。（**特别是回表主键索引的过程能体现**）
3. 优化1：索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁。
4. 优化2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁。
5. 一个bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。



​	等值查询的情况，对于非唯一索引，会一直遍历到不满足的行（**不唯一，需要继续找**）；而对于唯一索引，因为键值的唯一性，遍历到对应的区间就停下。（条件相等的位置或者所在的间隙位置）

> 假设当前插入的数据为：`insert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);`
>
> 条件相等的位置：`where id = 5`，所加的间隙锁就是(0,5]，不会继续找到10。同时根据优化1，会退化为行锁id=5。
>
> 所在的间隙位置:`where id =7`，所加的间隙锁就是(5,10],，不会继续找到15。



​	**如果没有索引，会做全表扫描。会锁住所有的行，也会在所有间隙加锁，达到锁全表的效果。**

##### 缺点

​	间隙锁的引入，可能会导致同样的语句锁住更大的范围，这其实是影响了并发度的。



[非唯一索引上存在等值的例子](https://blog.hellowood.dev/2019/01/07/MySQL-%E4%B8%AD%E5%85%B3%E4%BA%8Egap-lock-next-key-lock-%E7%9A%84%E4%B8%80%E4%B8%AA%E9%97%AE%E9%A2%98/)来说明真正的间隙锁

按照mysql45讲第21讲的表来说明

```mysql
CREATE TABLE `t` (
`id` int(11) NOTNULL,
`c` int(11) DEFAULTNULL,
`d` int(11) DEFAULTNULL,
PRIMARY KEY (`id`),
KEY `c` (`c`)
) ENGINE=InnoDB;

insert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);

```

**session A执行**

根据分析mysql45讲分析可知，因为是倒序的，所以非唯一索引c上的锁为(5,25)。

​	由于是 order by c desc，第一个要定位的是索引 c 上“最右边的”c=20 的行，所以会加上间隙锁 (20,25) 和 next-key lock (15,20]。在索引 c 上向左遍历，要扫描到 c=10 才停下来，所以 next-key lock 会加到 (5,10]，这正是阻塞 session B 的 insert 语句的原因。在扫描过程中，c=20、c=15、c=10 这三行都存在值，由于是 select *，所以会在主键 c上加三个行锁。

​	加锁是加在索引上的，而c和id是不同的索引。 索引c上，20,15,10这3行都被访问了，应该都加了行锁。 然后由于是 select * 所以需要回表，id 上的对应行也会被加锁。 但是**c=10这行并不满足条件**，只是在索引c上被访问了，加了锁，不满足条件就丢弃了，**不会回表**，所以id上没有被访问，也不会加锁。 即c上有20,15,10加锁，**id上只有20,15加锁**。（原则2）

```mysql
begin;
select * from t where c>=15 and c <=20 order by c desc lock in share mode;
```

**session B**

执行如下，(2,25)落在了(25,25)上面，所以会被锁住。(1000,5)落在了(5,5)下面，也会被锁住。

(2,5)落在了(5,5)上面，不会被锁住；(26,25)落在了(25,25)下面,，也不会被锁住。

因为先搜索的是索引c的树，以c为主。

```mysql
insert into t values(2,25,1);	//block
insert into t values(1000,5,1);	//block
insert into t values(2,5,1);	//success
insert into t values(26,25,1);	//success
```



#### 总结

​	全局锁主要用在逻辑备份过程中。对于全部是InnoDB引擎的库，我建议你选择使用–single transaction参数，对应用会更友好。



## order by实现原理

### 全字段排序

​	MySQL会给每个线程分配一块内存用于排序，称为sort_buffer，由参数sort_buffer_size确定。

​	order by过程如下（假设有索引）：

1. 根据索引找到主键id，并把主键id上**需要查询的字段放到sort_buffer**中。
2. 重复这个过程直到查询不满足
3. 对sort buffer中数据按照sql语句所要求的字段进行**快速排序**，并把结果返回给客户端



​	排序可能在内存中完成，也可能需要使用外部排序，这取决于排序所需的内存和参sort_buffer_size。如果要排序的数据量小于sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。外部排序排序一般使用**归并排序**。

![1679462843578](C:\Users\Chenhui\AppData\Roaming\Typora\typora-user-images\1679462843578.png)

### rowid排序

​	如果MySQL认为排序的单行长度太大，则会采用rowid。rowid排序会多查询一次原表获取查询数据。`max_length_for_sort_data`是MySQL中专门控制用于排序的行数据的长度的一个参数。它的意思是，如果单行的长度超过这个值，MySQL就认为单行太大，采用rowid排序算法。

​	rowid排序算法放入sort_buffer的字段，只有要排序的列和主键id。此时在sort buffer执行完排序后缺少查询的数据，需要多一个回表查询缺失数据的步骤。

![1679462801710](C:\Users\Chenhui\AppData\Roaming\Typora\typora-user-images\1679462801710.png)

### 总结

​	如果MySQL实在是担心排序内存太小，会影响排序效率，才会采用rowid排序算法，这样排序过程中一次可以排序更多行，但是需要再回到原表去取数据。

​	如果MySQL认为内存足够大，会优先选择全字段排序，把需要的字段都放到sort_buffer中，这样排序后就会直接从内存里面返回查询结果了，不用再回到原表去取数据。

​	这也就体现了MySQL的一个设计思想：**如果内存够，就要多利用内存，尽量减少磁盘访问**。对于InnoDB表来说，rowid排序会要求回表多造成磁盘读，因此不会被优先选择。

## 主备原理

主库A内部有一个线程，专门用于服务备库B的这个长连接。一个事务日志同步的完整过程是这样的：

1. 在备库B上通过`change master`命令，设置主库A的IP、端口、用户名、密码，以及要从哪个位置开始请求binlog，这个位置包含文件名和日志偏移量。
2. 在备库B上执行`start slave`命令，这时候备库会启动两个线程，就是图中的io_thread和sql_thread。其中io_thread负责与主库建立连接。
3. 主库A校验完用户名、密码后，开始按照备库B传过来的位置，从本地读取binlog，发给B。
4. 备库B拿到binlog后，写到本地文件，称为中转日志（relay log）。
5. sql_thread读取中转日志，解析出日志里的命令，并执行。	

​	start slave命令是用来启动MySQL复制从服务器的命令，它会告诉从服务器开始从主服务器同步数据。使用start slave命令之前，还需要使用change master命令指定要连接的主服务器和认证信息，以及确保主服务器正在运行。

change master命令

```mysql
//基于位点
CHANGE MASTER TO
MASTER_HOST=$host_name
MASTER_PORT=$port
MASTER_USER=$user_name
MASTER_PASSWORD=$password
MASTER_LOG_FILE=$master_log_name
MASTER_LOG_POS=$master_log_pos
```



​	图中画出的就是一个update语句在节点A执行，然后同步到节点B的完整流程图。

![1679209709978](C:\Users\Chenhui\AppData\Roaming\Typora\typora-user-images\1679209709978.png)

### 主备切换

​	正常情况下，只要主库执行更新生成的所有binlog，都可以传到备库并被正确地执行，备库就能达到跟主库一致的状态，这就是**最终一致性**。MySQL高可用系统的基础，就是**主备切换逻辑**。

​	主备延迟，就是同一个事务在备库执行完成的时间和主库执行完成的时间之间的差值。在备库上执行`showslave status`命令，它的返回结果里面会显示。**主备延迟最直接的表现是备库消费中转日志（relay log）的速度，比主库生产binlog的速度要慢。**

导致主备延迟的原因：

1. 备库所在机器的性能要比主库所在的机器性能差
2. 备库的压力大
3. 大事务
4. 备库的并行复制能力

导致大事务的原因：

1. 一次性地用delete语句删除太多数据
2. 大表DDL



主备切换策略：

1. 可靠性优先策略
2. 可用性优先策略



​	可靠性优先策略存在不可用时间，这段时间内主库和备库都是readonly状态。而可用性优先策略可能出现**数据不一致**的情况， 使用row格式的binlog时，数据不一致的问题更容易被发现。而使用mixed或者statement格式的binlog时，数据很可能悄悄地就不一致了。

​	**推荐使用可靠性优先的策略。毕竟保证数据准确，应该是数据库服务的底线。在这个基础上，通过减少主备延迟，提升系统的可用性。**



### 并行复制策略

![1679902122699](C:\Users\Chenhui\AppData\Roaming\Typora\typora-user-images\1679902122699.png)

​	图中备库上sql_thread更新数据(DATA)的逻辑。如果是用单线程的话，就会导致备库应用日志不够快，造成主备延迟。而所有的多线程复制机制，都是要把只有一个线程的sql_thread，拆成多个线程。

![1679903630864](C:\Users\Chenhui\AppData\Roaming\Typora\typora-user-images\1679903630864.png)

coordinator在分发的时候，需要满足以下这两个**基本要求**：

1. 不能造成更新覆盖。这就要求更新同一行的两个事务，必须被分发到同一个worker中。
2. 同一个事务不能被拆开，必须放到同一个worker中。



并行复制策略：

1. 按照表分发
2. 按照行分发（如果两个事务没有更新相同的行，它们在备库上可以并行执行。这个模式要求**binlog格式必须是row**。）
3. 按照库分发
4. 模拟主备并行方式（MariaDB，MySQL 5.7实现，**所有处于commit”状态的事务可以并行**）

MySQL 5.7并行复制策略的思想是：
1. 同时处于prepare状态的事务，在备库执行时是可以并行的；
2. 处于prepare状态的事务，与处于commit状态的事务之间，在备库执行时也是可以并行的。



### 一主多从的切换

​	相比于一主一备的切换流程，一主多从结构在切换完成后，A’会成为新的主库，从库B、C、D也要改接到A’。正是由于多了从库B、C、D重新指向的这个过程，所以主备切换的复杂性也相应增加了。

![1679906345740](C:\Users\Chenhui\AppData\Roaming\Typora\typora-user-images\1679906345740.png)

#### 基于位点的主备切换

​	当我们把节点B设置成节点A’的从库的时候，需要执行一条change master命令：

```mysql
CHANGE MASTER TO
MASTER_HOST=$host_name
MASTER_PORT=$port
MASTER_USER=$user_name
MASTER_PASSWORD=$password
MASTER_LOG_FILE=$master_log_name
MASTER_LOG_POS=$master_log_pos
```

​	最后两个参数MASTER_LOG_FILE和MASTER_LOG_POS表示，要从主库的`master_log_name`文件的`master_log_pos`这个位置的日志继续同步。而这个位置就是我们所说的同步位点，也就是主库对应的文件名和日志偏移量。

​	原来节点B是A的从库，本地记录的也是A的位点。但是相同的日志，A的位点和A’的位点是不同的。因此，**从库B要切换的时候，就需要先经过“找同步位点”这个逻辑**。考虑到切换过程中不能丢数据，所以我们找位点的时候，**总是要找一个“稍微往前”的，然后再通过判断跳过那些在从库B上已经执行过的事务。**

通常情况下，我们在切换任务的时候，要先主动跳过这些错误，有两种常用的方法。

1. 主动跳过一个事务。
2. 通过设置slave_skip_errors参数，直接设置跳过指定的错误。



#### 基于GTID的主备切换

在GTID模式下，备库B要设置为新主库A’的从库的语法如下：

```mysql
CHANGE MASTER TO
MASTER_HOST=$host_name
MASTER_PORT=$port
MASTER_USER=$user_name
MASTER_PASSWORD=$password
master_auto_position=1
```

其中，master_auto_position=1就表示这个主备关系使用的是GTID协议。

现在这个时刻，实例A’的GTID集合记为set_a，实例B的GTID集合记为set_b。我们在实例B上执行start slave命令，取binlog的逻辑是这样的：

1. 实例B指定主库A’，基于主备协议建立连接。
2. 实例B把set_b发给主库A’。
3. 实例A’算出set_a与set_b的差集，也就是所有存在于set_a，但是不存在于set_b的GITD的集合，判断A’本地是否包含了这个差集需要的所有binlog事务。
   a. **如果不包含，表示A’已经把实例B需要的binlog给删掉了，直接返回错误；**
   b. 如果确认全部包含，A’从自己的binlog文件里面，找出第一个不在set_b的事务，发给B；
4. 之后就从这个事务开始，往后读文件，按顺序取binlog发给B去执行。



​	这个逻辑里面包含了一个设计思想：**在基于GTID的主备关系里，系统认为只要建立主备关系，就必须保证主库发给备库的日志是完整的。**因此，如果实例B需要的日志已经不存在，A’就拒绝把日志发给B。

​	这跟基于位点的主备协议不同。基于位点的协议，是由备库决定的，备库指定哪个位点，主库就发哪个位点，不做日志的完整性判断。



#### 总结

​	基于GTID的主备切换不是不需要找位点了，而是找位点这个工作，在备库内部就已经自动完成了。



### 读写分离

​	由于主从可能存在延迟，客户端执行完一个更新事务后马上发起查询，如果查询选择的是从库的话，就有可能读到刚刚的事务更新之前的状态。这种“在从库上会读到系统的一个过期状态”的现象称之为“过期读”。

为了解决过期读，常见的解决办法有：

1. 强制走主库方案
2. sleep方案
3. 判断主备无延迟方案
4. 配合semi-sync方案
5. 等主库位点方案
6. 等主库位点方案



​	强制走主库方案就是将查询请求做分类，允许过期读走从库，不允许走主库。

​	判断主备无延迟一般可以通过判断seconds_behind_master是否为0、对比位点、对比GTID集合这三种方法。

​	等主库位点和等主库位点方案的思想是在从库查询位点或者GTID来判断备库是否已经执行了主库的binlog（主动查询）。



#### 总结

​	先在客户端对请求做分类，区分哪些请求可以接受过期读，而哪些请求完全不能接受过期；然后，对于不能接受过期读的语句，再使用等GTID或等位点的方案。



## 其他知识点

### count（）执行逻辑

​	count()是一个聚合函数，对于返回的结果集，一行行地判断，如果**count函数的参数不是NULL**，累计值就加1，否则不加。最后返回累计值。

​	count(*)、count(主键id)和count(1) 都表示返回满足条件的结果集的总行数；而count(字段），则表示返回满足条件的数据行里面，参数“字段”不为NULL的总个数。

​	按照效率排序的话，count(字段)<count(主键id) <count(1)≈count(* )，所以我建议你，尽量使count(*)。



### 易忘点

回表是一行行主键索引的

MRR优化，顺序性的查询主键id。

LRU分为young和old区域是为了防止全量扫描大表的时候，被大表占用了buffer pool导致查询命中率低。短时间被多次访问的页，存在old中，不影响buffer pool的使用。

explain中的Using index表示用到了索引覆盖。

explain中extra字段介绍

> Using temporary，表示使用了临时表
>
> Using index，表示用到了索引覆盖
>
> Using filesort，表示需要排序

insert …select 是很常见的在两个表之间拷贝数据的方法。你需要注意，在可重复读隔离级别下，这个语句会给select的表里扫描到的记录和间隙加读锁。